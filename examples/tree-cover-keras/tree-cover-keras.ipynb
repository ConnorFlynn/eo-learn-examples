{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook for training a U-net deep learning network to predict tree cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a toy example for training a deep learning architecture for semantic segmentation of satellite images using `eo-learn` and `keras`. The example showcases tree cover over prediction over an area in Framce. The ground-truth data is retrieved from the [EU tree cover density (2015)](https://land.copernicus.eu/pan-european/high-resolution-layers/forests/view) through [Geopedia](http://www.geopedia.world/#T235_L2081_x449046.043261205_y6052157.300792162_s15_b17).\n",
    "\n",
    "The workflow is as foolows:\n",
    " * input the area-of-interest (AOI)\n",
    " * split the AOI into small manageable eopatches\n",
    " * for each eopatch:\n",
    "     * download RGB bands form Sentinel-2 L2A products using Sentinel-Hub for the 2017 year \n",
    "     * retrieve corresponding ground-truth from Geopedia using a WMS request\n",
    "     * compute the median values for the RGB bands over the time-interval\n",
    "     * save to disk\n",
    "     * select a 256x256 patch with corresponding ground-truth to be used for training/validating the model\n",
    " * train and validate a U-net\n",
    " \n",
    "This example can easily be expanded to:\n",
    " * larger AOIs;\n",
    " * include more/different bands/indices, such as NDVI\n",
    " * include Sentinel-1 images (after harmonisation with Sentinel-2)\n",
    " \n",
    "The notebook requires `Keras` with `tensorflow` back-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path as op\n",
    "import itertools\n",
    "\n",
    "from eolearn.io import *\n",
    "from eolearn.core import EOTask, EOPatch, EOWorkflow, FeatureType, SaveToDisk, OverwritePermission\n",
    "from sentinelhub import BBox, CRS, BBoxSplitter, MimeType, ServiceType, CustomUrlParam, GeopediaWmsRequest\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import geopandas\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sentinel-hub.com/faq/where-get-instance-id\n",
    "INSTANCE_ID = os.environ.get('INSTANCE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global image request parameters\n",
    "time_interval = ('2017-01-01', '2017-12-31')\n",
    "img_width = 256\n",
    "img_height = 256\n",
    "maxcc = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the AOI and split into bboxes\n",
    "crs = CRS.UTM_31N\n",
    "aoi = geopandas.read_file('../../example_data/eastern_france.geojson')\n",
    "aoi = aoi.to_crs(crs={'init':CRS.ogc_string(crs)})\n",
    "aoi_shape = aoi.geometry.values.tolist()[-1]\n",
    "\n",
    "bbox_splitter = BBoxSplitter([aoi_shape], crs, (19, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set raster_value conversions for our Geopedia task\n",
    "# see more about how to do this here:\n",
    "\n",
    "raster_value = {\n",
    "    '0%': (0, [0, 0, 0, 0]),\n",
    "    '10%': (1, [163, 235,  153, 255]),\n",
    "    '30%': (2, [119, 195,  118, 255]),\n",
    "    '50%': (3, [85, 160, 89, 255]),\n",
    "    '70%': (4, [58, 130, 64, 255]),\n",
    "    '90%': (5, [36, 103, 44, 255])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "tree_cmap = mpl.colors.ListedColormap(['#F0F0F0', '#A2EB9B', '#77C277', '#539F5B', '#388141', '#226528'])\n",
    "tree_cmap.set_over('white')\n",
    "tree_cmap.set_under('white')\n",
    "\n",
    "bounds = np.arange(-0.5, 6, 1).tolist()\n",
    "tree_norm = mpl.colors.BoundaryNorm(bounds, tree_cmap.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a task for calculating a median pixel value\n",
    "class MedianPixel(EOTask):\n",
    "    \"\"\"\n",
    "    The task returns a pixelwise median value from a time-series and stores the results in a timeless data array.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature, feature_out):\n",
    "        self.feature_type, self.feature_name = next(self._parse_features(feature)())\n",
    "        self.feature_type_out, self.feature_name_out = next(self._parse_features(feature_out)())\n",
    "\n",
    "    def execute(self, eopatch):\n",
    "        eopatch.add_feature(self.feature_type_out, self.feature_name_out, \n",
    "                            np.median(eopatch[self.feature_type][self.feature_name], axis=0))\n",
    "        return eopatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tasks\n",
    "# task to get S2 L2A images\n",
    "input_task = S2L2AWCSInput('TRUE-COLOR-S2-L2A', resx='10m', resy='10m', maxcc=0.2)\n",
    "# task to get ground-truth from Geopedia\n",
    "geopedia_data = AddGeopediaFeature((FeatureType.MASK_TIMELESS, 'TREE_COVER'), 'ttl2275', 'QP', raster_value)\n",
    "# task to compute median values\n",
    "get_median_pixel = MedianPixel((FeatureType.DATA, 'TRUE-COLOR-S2-L2A'), feature_out=(FeatureType.DATA_TIMELESS, 'MEDIAN_PIXEL'))\n",
    "# task to save to disk\n",
    "save = SaveToDisk(op.join('data', 'eopatch'), overwrite_permission=OverwritePermission.OVERWRITE_PATCH, compress_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize workflow\n",
    "workflow = EOWorkflow.make_linear_workflow(input_task, geopedia_data, get_median_pixel, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a function to run this workflow on a single bbox\n",
    "def execute_workflow(index):\n",
    "    bbox = bbox_splitter.bbox_list[index]\n",
    "    info = bbox_splitter.info_list[index]\n",
    "    \n",
    "    patch_name = 'eopatch_{0}_row-{1}_col-{2}'.format(index, \n",
    "                                                      info['index_x'], \n",
    "                                                      info['index_y'])\n",
    "    \n",
    "    results = workflow.execute({input_task:{'bbox':bbox, 'time_interval':time_interval},\n",
    "                                save:{'eopatch_folder':patch_name}\n",
    "                               })\n",
    "    return list(results.values())[-1]\n",
    "    del results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test workflow on an example patch and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 168\n",
    "example_patch = execute_workflow(idx)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = example_patch.data_timeless['MEDIAN_PIXEL']\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(2.5*mp)\n",
    "tc = example_patch.mask_timeless['TREE_COVER']\n",
    "plt.imshow(tc[...,0], vmin=0, vmax=5, alpha=.5, cmap=tree_cmap)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run workflow an all patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run over multiple bboxes\n",
    "subset_idx = len(bbox_splitter.bbox_list)\n",
    "x_train_raw = np.empty((subset_idx, 256, 256, 3))\n",
    "y_train_raw = np.empty((subset_idx, 256, 256, 1))\n",
    "pbar = tqdm(total=subset_idx)\n",
    "for idx in range(0, subset_idx):\n",
    "    patch = execute_workflow(idx)\n",
    "    x_train_raw[idx] = patch.data_timeless['MEDIAN_PIXEL'][20:276,0:256,:]\n",
    "    y_train_raw[idx] = patch.mask_timeless['TREE_COVER'][20:276,0:256,:]\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create training and validation data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normalization and augmentation\n",
    "img_mean = np.mean(x_train_raw, axis=(0, 1, 2))\n",
    "img_std = np.std(x_train_raw, axis=(0, 1, 2))\n",
    "x_train_mean = x_train_raw - img_mean\n",
    "x_train = x_train_mean - img_std\n",
    "\n",
    "train_gen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rotation_range=180)\n",
    "\n",
    "y_train = to_categorical(y_train_raw, len(raster_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set up U-net model using Keras (tensorflow back-end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "#from https://www.kaggle.com/lyakaap/weighing-boundary-pixels-loss-script-by-keras2\n",
    "# weight: weighted tensor(same shape with mask image)\n",
    "def weighted_bce_loss(y_true, y_pred, weight):\n",
    "    # avoiding overflow\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits\n",
    "    loss = (1. - y_true) * logit_y_pred + (1. + (weight - 1.) * y_true) * \\\n",
    "    (K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n",
    "    return K.sum(loss) / K.sum(weight)\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred, weight):\n",
    "    smooth = 1.\n",
    "    w, m1, m2 = weight * weight, y_true, y_pred\n",
    "    intersection = (m1 * m2)\n",
    "    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n",
    "    loss = 1. - K.sum(score)\n",
    "    return loss\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd number\n",
    "    averaged_mask = K.pool2d(\n",
    "            y_true, pool_size=(11, 11), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    border = K.cast(K.greater(averaged_mask, 0.005), 'float32') * K.cast(K.less(averaged_mask, 0.995), 'float32')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight += border * 2\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = weighted_bce_loss(y_true, y_pred, weight) + \\\n",
    "    weighted_dice_loss(y_true, y_pred, weight)\n",
    "    return loss\n",
    "\n",
    "def unet(input_size):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = merge([drop4,up6], mode = 'concat', concat_axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = merge([conv3,up7], mode = 'concat', concat_axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = merge([conv2,up8], mode = 'concat', concat_axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = merge([conv1,up9], mode = 'concat', concat_axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(len(raster_value), 1, activation = 'softmax')(conv9)\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = weighted_bce_dice_loss, metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = unet(input_size=(256, 256, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "batch_size = 16\n",
    "model.fit_generator(\n",
    "        train_gen.flow(x_train, y_train, batch_size=batch_size),\n",
    "        steps_per_epoch=len(x_train),\n",
    "        epochs=20,\n",
    "        verbose=1)\n",
    "model.save(op.join('model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validate model and show some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one example (image, label, prediction)\n",
    "idx = 4\n",
    "p = np.argmax(model.predict(np.array([x_train[idx]])), axis=3)\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax1.imshow(x_train_raw[idx])\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax2.imshow(y_train_raw[idx][:,:,0])\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "ax3.imshow(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show image confusion matrix\n",
    "predictions = np.argmax(model.predict(x_train), axis=3)\n",
    "cnf_matrix = confusion_matrix(y_train_raw.reshape(len(y_train_raw) * 256 * 256, 1), predictions.reshape(len(predictions) * 256 * 256, 1))\n",
    "plot_confusion_matrix(cnf_matrix, raster_value.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
